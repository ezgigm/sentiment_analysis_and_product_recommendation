{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ezgigm/pokemon/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "brprw0cBT-2C",
    "outputId": "46985094-e8ae-4b42-e732-51281a23ec41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IAZGL6_fUFNq",
    "outputId": "4ba216f4-ada5-4e48-f34f-861c7f41ca1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "-N6J8qsxUyKO",
    "outputId": "a004ffd8-a8ca-47cf-8eac-29fc2eebc73d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 22.6MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 6.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 30kB 7.5MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 51kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 61kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 71kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 81kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 92kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 102kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 112kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 122kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 133kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 143kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 153kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 163kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 174kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 184kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 194kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 204kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 215kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 225kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 235kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 245kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 256kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 266kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 276kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 286kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 296kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 307kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 317kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 327kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 337kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 348kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 358kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 368kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 378kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 389kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 399kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 409kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 419kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 430kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 440kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 450kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 460kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 471kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 481kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 491kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 501kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 512kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 522kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 532kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 542kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 552kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 563kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 573kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 583kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 593kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 604kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 614kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 624kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 634kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 645kB 8.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 23.8MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 50.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 50.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=7db7e337cca6d4e824b4a107364d4cf0f5354cc3412fe06cd49a3a310893a39d\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "idaZ0h77U2Ow",
    "outputId": "51d12694-c4a4-4c1a-c60a-6dfb19511a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=015c2289431faadd61ec146b87c9befa9c39399e107e9b3bf867335327c049d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WLiUUD8U9zJ"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgnR737hVFIJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "jVbeyyADWb9E",
    "outputId": "1f1538be-fe38-4707-e3c7-c2ebc275799b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrgHKJkuViFo"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/My Drive/train/train.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cDaK2F8IXob8",
    "outputId": "678a6597-ee6e-42c3-a644-b1d079b8c902"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_clean    0\n",
       "sentiment       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76898PDxXtUm"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSYeOlrUX74v"
   },
   "outputs": [],
   "source": [
    "sentences = df.review_clean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_TCmxQIYUV2"
   },
   "outputs": [],
   "source": [
    "label = df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "d7d24741db9a4cf59b701080fdc708e4",
      "fdbf18eb52a84873b92871328d41e6c6",
      "3560dab447db4fd7ae693b3c71bc110e",
      "73ac4055d22b43ceb0975a78af928f21",
      "943ed645d2084cfaa302247bcf5ea00a",
      "28f9d6f70b774a68882e79c809e1f5be",
      "881a66794b814e548fe39b2ccd7d80b1",
      "0e28fab1deae4b509e1b2e74dd362c3d"
     ]
    },
    "colab_type": "code",
    "id": "mzSyvFA5YYZp",
    "outputId": "85bc93ad-cb0c-4c49-e64a-4452c0ca2d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d24741db9a4cf59b701080fdc708e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "UAytA7woYd-F",
    "outputId": "40154b88-d281-4409-dec4-e12804015215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  this is a cute little book that is fairly short and easy to read this author fell in love with a silver camper bought it refurbished it and off she went to camp for six months those six months turned into a trip of a life time and a move across the country by the time it was over\n",
      "\n",
      "i give it a little bit lower score than others because i didnt find it laugh out loud hilarious it was funny in a cute way and for someone who owns a camper and goes camping often much of this book is right on target about how not to  and it was right on target about life in a campground\n",
      "\n",
      "overall this is an enjoyable book it is cute and clever it is not a long read and it is right on target in many ways \n",
      "enjoy\n",
      "Tokenized:  ['this', 'is', 'a', 'cute', 'little', 'book', 'that', 'is', 'fairly', 'short', 'and', 'easy', 'to', 'read', 'this', 'author', 'fell', 'in', 'love', 'with', 'a', 'silver', 'camp', '##er', 'bought', 'it', 'refurbished', 'it', 'and', 'off', 'she', 'went', 'to', 'camp', 'for', 'six', 'months', 'those', 'six', 'months', 'turned', 'into', 'a', 'trip', 'of', 'a', 'life', 'time', 'and', 'a', 'move', 'across', 'the', 'country', 'by', 'the', 'time', 'it', 'was', 'over', 'i', 'give', 'it', 'a', 'little', 'bit', 'lower', 'score', 'than', 'others', 'because', 'i', 'didn', '##t', 'find', 'it', 'laugh', 'out', 'loud', 'hilarious', 'it', 'was', 'funny', 'in', 'a', 'cute', 'way', 'and', 'for', 'someone', 'who', 'owns', 'a', 'camp', '##er', 'and', 'goes', 'camping', 'often', 'much', 'of', 'this', 'book', 'is', 'right', 'on', 'target', 'about', 'how', 'not', 'to', 'and', 'it', 'was', 'right', 'on', 'target', 'about', 'life', 'in', 'a', 'campground', 'overall', 'this', 'is', 'an', 'enjoyable', 'book', 'it', 'is', 'cute', 'and', 'clever', 'it', 'is', 'not', 'a', 'long', 'read', 'and', 'it', 'is', 'right', 'on', 'target', 'in', 'many', 'ways', 'enjoy']\n",
      "Token IDs:  [2023, 2003, 1037, 10140, 2210, 2338, 2008, 2003, 7199, 2460, 1998, 3733, 2000, 3191, 2023, 3166, 3062, 1999, 2293, 2007, 1037, 3165, 3409, 2121, 4149, 2009, 18662, 2009, 1998, 2125, 2016, 2253, 2000, 3409, 2005, 2416, 2706, 2216, 2416, 2706, 2357, 2046, 1037, 4440, 1997, 1037, 2166, 2051, 1998, 1037, 2693, 2408, 1996, 2406, 2011, 1996, 2051, 2009, 2001, 2058, 1045, 2507, 2009, 1037, 2210, 2978, 2896, 3556, 2084, 2500, 2138, 1045, 2134, 2102, 2424, 2009, 4756, 2041, 5189, 26316, 2009, 2001, 6057, 1999, 1037, 10140, 2126, 1998, 2005, 2619, 2040, 8617, 1037, 3409, 2121, 1998, 3632, 13215, 2411, 2172, 1997, 2023, 2338, 2003, 2157, 2006, 4539, 2055, 2129, 2025, 2000, 1998, 2009, 2001, 2157, 2006, 4539, 2055, 2166, 1999, 1037, 29144, 3452, 2023, 2003, 2019, 22249, 2338, 2009, 2003, 10140, 1998, 12266, 2009, 2003, 2025, 1037, 2146, 3191, 1998, 2009, 2003, 2157, 2006, 4539, 1999, 2116, 3971, 5959]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "UHg8SsfQ3k8-",
    "outputId": "c26b68de-73b1-47a2-bd65-93dfd0c95b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  this is a cute little book that is fairly short and easy to read this author fell in love with a silver camper bought it refurbished it and off she went to camp for six months those six months turned into a trip of a life time and a move across the country by the time it was over\n",
      "\n",
      "i give it a little bit lower score than others because i didnt find it laugh out loud hilarious it was funny in a cute way and for someone who owns a camper and goes camping often much of this book is right on target about how not to  and it was right on target about life in a campground\n",
      "\n",
      "overall this is an enjoyable book it is cute and clever it is not a long read and it is right on target in many ways \n",
      "enjoy\n",
      "Token IDs: [101, 2023, 2003, 1037, 10140, 2210, 2338, 2008, 2003, 7199, 2460, 1998, 3733, 2000, 3191, 2023, 3166, 3062, 1999, 2293, 2007, 1037, 3165, 3409, 2121, 4149, 2009, 18662, 2009, 1998, 2125, 2016, 2253, 2000, 3409, 2005, 2416, 2706, 2216, 2416, 2706, 2357, 2046, 1037, 4440, 1997, 1037, 2166, 2051, 1998, 1037, 2693, 2408, 1996, 2406, 2011, 1996, 2051, 2009, 2001, 2058, 1045, 2507, 2009, 1037, 2210, 2978, 2896, 3556, 2084, 2500, 2138, 1045, 2134, 2102, 2424, 2009, 4756, 2041, 5189, 26316, 2009, 2001, 6057, 1999, 1037, 10140, 2126, 1998, 2005, 2619, 2040, 8617, 1037, 3409, 2121, 1998, 3632, 13215, 2411, 2172, 1997, 2023, 2338, 2003, 2157, 2006, 4539, 2055, 2129, 2025, 2000, 1998, 2009, 2001, 2157, 2006, 4539, 2055, 2166, 1999, 1037, 29144, 3452, 2023, 2003, 2019, 22249, 2338, 2009, 2003, 10140, 1998, 12266, 2009, 2003, 2025, 1037, 2146, 3191, 1998, 2009, 2003, 2157, 2006, 4539, 1999, 2116, 3971, 5959, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        max_length = 512          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "9Vm0MhHKYkIa",
    "outputId": "289f9f79-9abf-44c2-fae0-a2afb3a1a6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  this is a cute little book that is fairly short and easy to read this author fell in love with a silver camper bought it refurbished it and off she went to camp for six months those six months turned into a trip of a life time and a move across the country by the time it was over\n",
      "\n",
      "i give it a little bit lower score than others because i didnt find it laugh out loud hilarious it was funny in a cute way and for someone who owns a camper and goes camping often much of this book is right on target about how not to  and it was right on target about life in a campground\n",
      "\n",
      "overall this is an enjoyable book it is cute and clever it is not a long read and it is right on target in many ways \n",
      "enjoy\n",
      "Token IDs: [101, 2023, 2003, 1037, 10140, 2210, 2338, 2008, 2003, 7199, 2460, 1998, 3733, 2000, 3191, 2023, 3166, 3062, 1999, 2293, 2007, 1037, 3165, 3409, 2121, 4149, 2009, 18662, 2009, 1998, 2125, 2016, 2253, 2000, 3409, 2005, 2416, 2706, 2216, 2416, 2706, 2357, 2046, 1037, 4440, 1997, 1037, 2166, 2051, 1998, 1037, 2693, 2408, 1996, 2406, 2011, 1996, 2051, 2009, 2001, 2058, 1045, 2507, 2009, 1037, 2210, 2978, 2896, 3556, 2084, 2500, 2138, 1045, 2134, 2102, 2424, 2009, 4756, 2041, 5189, 26316, 2009, 2001, 6057, 1999, 1037, 10140, 2126, 1998, 2005, 2619, 2040, 8617, 1037, 3409, 2121, 1998, 3632, 13215, 2411, 2172, 1997, 2023, 2338, 2003, 2157, 2006, 4539, 2055, 2129, 2025, 2000, 1998, 2009, 2001, 2157, 2006, 4539, 2055, 2166, 1999, 1037, 29144, 3452, 2023, 2003, 2019, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        max_length = 128          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oZFpjlnOYxmt",
    "outputId": "6023cfe3-b928-423b-a7e9-c01260f1eda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  512\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuIM_xVmaX4I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo",
    "outputId": "a3f2ca3f-f5f6-44ae-cc46-817cd69cfb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 128 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
    "# maximum training sentence length of 47...\n",
    "MAX_LEN = 128\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ojtKhuPatXT"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F92-y3sJawbO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use 80% for training and 20% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, label, \n",
    "                                                            random_state=2018, test_size=0.2)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, label,\n",
    "                                             random_state=2018, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdQSuZ94a7f2"
   },
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m5nE_wKbSCA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KwAKvngYbYSB",
    "outputId": "0cf0bbc3-e255-42b7-c193-2797f875abb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "Oh4ApNSjbcAS",
    "outputId": "57ba1d32-2ab5-4321-875f-aa6ea6b6d146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KekfSfNPbnR_"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPUUU-AnbuPd"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZM2FxwESb0Ru"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9EkYZgHb8nH"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rX2Qzc_7ws73"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_MUc3js8wu8T",
    "outputId": "b7b0aaee-6f57-4654-b28a-05c085b5929d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:01.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:16.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:31.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:46.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:01.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:16.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:31.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:46.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:01.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:16.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:31.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:46.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:01.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:16.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:31.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:46.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:01.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:16.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:31.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:46.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:01.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:16.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:31.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:46.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:02.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:17.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:32.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:47.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:02.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:17.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:32.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:47.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:02.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:17.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:32.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:47.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:02.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:17.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:32.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:47.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:02.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:17.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:12:31\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:15.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:30.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:45.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:00.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:15.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:30.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:45.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:00.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:15.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:30.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:45.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:00.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:15.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:30.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:44.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:59.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:14.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:29.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:44.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:00.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:15.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:30.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:45.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:00.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:15.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:12:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:15.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:30.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:45.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:00.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:15.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:30.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:45.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:00.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:15.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:30.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:45.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:00.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:15.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:30.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:45.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:00.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:15.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:30.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:45.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:00.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:15.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:30.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:45.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:00.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:15.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:12:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:14.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:29.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:44.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:59.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:43.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:58.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:13.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:28.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:43.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:58.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:13.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:28.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:43.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:58.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:13.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:28.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:43.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:58.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:13.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:28.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:43.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:58.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:13.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:28.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:43.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:58.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:13.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:12:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:14.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:29.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:44.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:59.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:44.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:59.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:14.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:29.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:44.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:59.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:14.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:29.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:44.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:59.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:14.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:29.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:44.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:59.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:14.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:29.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:44.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:59.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:14.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:29.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:44.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:59.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:14.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:12:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    \n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "        \n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Report the final accuracy for this validation run.\n",
    "   \n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zbIaxsi7b_M4",
    "outputId": "ad26d429-317b-4214-b95d-968fd2f0bb7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:31.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:46.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:01.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:16.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:31.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:46.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:01.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:16.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:31.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:46.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:01.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:16.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:32.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:47.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:02.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:17.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:32.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:47.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:02.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:17.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:32.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:47.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:02.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:17.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:32.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:47.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:02.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:17.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:32.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:47.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:03.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:18.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:33.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:48.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:03.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:18.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:33.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:48.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:03.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:18.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:33.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:48.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:03.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:18.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:33.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:48.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:03.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:18.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:12:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:15.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:30.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:45.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:00.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:15.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:30.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:45.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:00.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:15.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:30.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:45.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:00.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:15.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:30.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:45.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:00.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:15.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:30.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:45.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:00.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:15.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:30.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:45.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:00.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:15.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:12:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:15.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:30.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:45.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:00.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:15.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:30.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:45.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:00.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:15.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:30.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:45.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:00.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:15.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:30.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:45.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:00.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:15.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:30.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:45.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:00.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:15.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:30.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:45.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:00.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:15.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:12:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:44.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:59.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:14.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:29.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:44.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:59.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:14.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:29.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:44.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:59.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:14.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:29.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:44.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:59.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:14.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:29.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:44.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:59.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:14.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:29.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:44.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:58.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:13.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:12:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:14.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:29.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:44.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:59.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:44.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:59.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:14.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:29.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:44.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:59.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:14.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:29.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:44.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:59.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:14.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:29.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:44.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:59.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:14.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:29.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:44.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:59.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:14.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:29.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:44.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:59.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:14.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:12:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "logit_values =[]\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    total_logit = 0 #added part\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    total_logit += logits.item() #added\n",
    "    avg_logit = total_logit / len(validation_dataloader)\n",
    "    logit_values.append(avg_logit)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Average logit: {0:.2f}\".format(avg_logit))\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "6ipjiFXBmH-o",
    "outputId": "6da60d59-bdff-4532-d656-c9776bd8a2af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "colab_type": "code",
    "id": "xJ-thKwlcSoI",
    "outputId": "3eef0b58-7a41-4708-ee9d-111041266a2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weMCdBHitrOm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBz1qYfeuLlLrzLypc46Sh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e28fab1deae4b509e1b2e74dd362c3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28f9d6f70b774a68882e79c809e1f5be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3560dab447db4fd7ae693b3c71bc110e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28f9d6f70b774a68882e79c809e1f5be",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_943ed645d2084cfaa302247bcf5ea00a",
      "value": 231508
     }
    },
    "73ac4055d22b43ceb0975a78af928f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e28fab1deae4b509e1b2e74dd362c3d",
      "placeholder": "​",
      "style": "IPY_MODEL_881a66794b814e548fe39b2ccd7d80b1",
      "value": " 232k/232k [01:50&lt;00:00, 2.10kB/s]"
     }
    },
    "881a66794b814e548fe39b2ccd7d80b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "943ed645d2084cfaa302247bcf5ea00a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d7d24741db9a4cf59b701080fdc708e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3560dab447db4fd7ae693b3c71bc110e",
       "IPY_MODEL_73ac4055d22b43ceb0975a78af928f21"
      ],
      "layout": "IPY_MODEL_fdbf18eb52a84873b92871328d41e6c6"
     }
    },
    "fdbf18eb52a84873b92871328d41e6c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
