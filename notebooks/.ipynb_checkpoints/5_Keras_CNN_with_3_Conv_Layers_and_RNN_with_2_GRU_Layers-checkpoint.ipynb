{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Models - CNN with 3 Convolutional Layers and RNN with 2 GRU Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is the deep learning framework which is simple to use. I found it easier than to build a new model in torch, so I wanted to use and get results with Keras models. \n",
    "\n",
    "I tried 5 Keras models for this project. First 2 models can be found in this notebook. Other 3 models were run in Google Colab to get more fast results. So, they can be found in next notebook (number 6 notebook). This gives me a chance to run different models at the same time. \n",
    "\n",
    "### Aim of This Notebook:\n",
    "\n",
    "In this notebook, my aim is to get predictions with using Convolutional Neural Net models and Recurrent Neural Net models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "#tensorflow imports for keras\n",
    "import tensorflow\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv') # taking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>i am shocked  harrison at the very end gives p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the best self help book ive ever read half of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>quite interesting a time of intrigue and excit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>i love the bibliophile series  i saw that a eb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>this is a really great story filled with wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        review_clean  sentiment\n",
       "0  i am shocked  harrison at the very end gives p...          1\n",
       "1  the best self help book ive ever read half of ...          1\n",
       "2  quite interesting a time of intrigue and excit...          1\n",
       "3  i love the bibliophile series  i saw that a eb...          1\n",
       "4  this is a really great story filled with wonde...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) # last more cleaning to make sure for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data to Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure about using same sample data in each notebook, I always get same data and divide with same random state and test_size for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will divide text and target to prepare data to model. I will do it for both train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_data.sentiment\n",
    "train_texts = train_data.review_clean\n",
    "\n",
    "test_target = test_data.sentiment\n",
    "test_texts = test_data.review_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After here, I got inspired from this notebook which is the solution of one Kaggle compatition below,\n",
    "\n",
    "https://www.kaggle.com/muonneutrino/sentiment-analysis-with-amazon-reviews\n",
    "\n",
    "I used steps in this notebook to get baseline for Keras and I changed layers types and layers numbers to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I get together my text\n",
    "def converting_texts(texts):\n",
    "    collected_texts = []\n",
    "    for text in texts:\n",
    "        collected_texts.append(text)\n",
    "    return collected_texts\n",
    "        \n",
    "train_texts = converting_texts(train_texts)\n",
    "test_texts = converting_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to tokenize my text and padding sequences before modeling my data. I will use Keras proprocessing tools for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feat= 12000 #seting max features to define max number of tokenizer words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_feat)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "# updates internal vocabulary based on a list of texts\n",
    "# in the case where texts contains lists, we assume each entry of the lists to be a token\n",
    "# required before using texts_to_sequences or texts_to_matrix\n",
    "\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(test_texts)\n",
    "# transforms each text in texts to a sequence of integers\n",
    "# Only top num_words-1 most frequent words will be taken into account \n",
    "# Only words known by the tokenizer will be taken into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use batches productively, I need to turn my sequences to same lenght. I prefer to set everything to maximum lenght of the longest sentence in train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(train_ex) for train_ex in train_texts) #setting the max length\n",
    "\n",
    "# using pad_sequence tool from Keras\n",
    "# transforms a list of sequences to into a 2D Numpy array of shape \n",
    "# the maxlen argument for the length of the longest sequence in the list\n",
    "train_texts = pad_sequences(train_texts, maxlen=max_len)\n",
    "test_texts = pad_sequences(test_texts, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple model, convolutional neural nets were used with 64 embedding dimension. 3-convolutional layers used, first two have batch normalization and maximum pooling arguments. The last one has glabal maximum pooling. Results were passed to a dense layer and output for prediction.\n",
    "\n",
    "Batch normalizations normalize and scale inputs or activations by reducing the amount what the hidden unit values shift around. Max Pool downsamples the input representation by taking the maximum value over the window defined by pool size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    sequences = layers.Input(shape=(max_len,))\n",
    "    embedded = layers.Embedding(max_feat, 64)(sequences)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Model to My Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63983 samples, validate on 15996 samples\n",
      "Epoch 1/2\n",
      "63983/63983 [==============================]63983/63983 [==============================] - 963s 15ms/step - loss: 0.2943 - binary_accuracy: 0.9136 - val_loss: 0.2673 - val_binary_accuracy: 0.9155\n",
      "\n",
      "Epoch 2/2\n",
      "63983/63983 [==============================]63983/63983 [==============================] - 1005s 16ms/step - loss: 0.2185 - binary_accuracy: 0.9197 - val_loss: 0.2632 - val_binary_accuracy: 0.9175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x12ac76710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_texts, \n",
    "    train_target, \n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    validation_data=(test_texts, test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives me 0.263 loss value and 0.92% accuracy on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RNN model layers, I have embedding layer and also I used GRU layers which followed by 2 dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    sequences = layers.Input(shape=(max_len,))\n",
    "    embedded = layers.Embedding(max_feat, 64)(sequences)\n",
    "    x = layers.GRU(128, return_sequences=True)(embedded)\n",
    "    x = layers.GRU(128)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "rnn_model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Model to My Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63983 samples, validate on 15996 samples\n",
      "Epoch 1/1\n",
      "63983/63983 [==============================]63983/63983 [==============================] - 7834s 122ms/step - loss: 0.2611 - binary_accuracy: 0.9169 - val_loss: 0.2110 - val_binary_accuracy: 0.9203\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x130e73cf8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(\n",
    "    train_texts, \n",
    "    train_target, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(test_texts, test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for one epoch this takes too much time, so I opened this notebook in Google Colab and tried the epochs=2 version there and pasted results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was runned in Colab\n",
    "# rnn_model.fit(\n",
    "#     train_texts, \n",
    "#     train_target, \n",
    "#     batch_size=128,\n",
    "#     epochs=2,\n",
    "#     validation_data=(test_texts, test_target) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for 2 epoches;\n",
    "\n",
    "- loss: 0.1623\n",
    "- acc: 0.9371\n",
    "- val loss: 0.1615\n",
    "- val acc: 0.9377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will compare 5 Keras results in next notebook as total. But from here, I found RNN model more accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
