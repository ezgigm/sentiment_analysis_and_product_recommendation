{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is one of the user-friendly libraries in python and also FastText function is used for text classification. \n",
    "\n",
    "## Aim of This Notebook\n",
    "\n",
    "My aim in this notebook is to find a baseline with torch model, then getting better results with changing some parameters.\n",
    "\n",
    "Preparing data for modeling steps and all details about modeling steps can be found in this notebook. Also, future improvement plans were added to the end of the notebook. \n",
    "\n",
    "### Metric : \n",
    "\n",
    "As metric, I prefer to look at loss values for train and validation data. But, I also see the accuracy to interpret my results in more smarter way.\n",
    "\n",
    "### Best Results of This Notebook:\n",
    "\n",
    "91.66% accuracy with 0.380 loss for train, 91.68% accucary with 0.305 loss for validation obtained. Detailed parameters and values can be found in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe and series \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn imports for modeling part\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# To plot\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline    \n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "# torch model\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0520 13:01:43.983011 4716817856 file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#for text augmentation\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv') # taking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>A2LSKD2H9U8N0J</td>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>pretty good story, a little exaggerated, but i...</td>\n",
       "      <td>pretty good story</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>pretty good story a little exaggerated but i l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2014-05-26</td>\n",
       "      <td>A2QP13XTJND1QS</td>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>if you've read other max brand westerns, you k...</td>\n",
       "      <td>A very good book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>if youve read other max brand westerns you kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>A8WQ7MAG3HFOZ</td>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>love max, always a fun twist</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>love max always a fun twist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-03-03</td>\n",
       "      <td>A1E0MODSRYP7O</td>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>as usual for him, a good book</td>\n",
       "      <td>a good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>as usual for him a good book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>2015-09-10</td>\n",
       "      <td>AYUTCGVSM1H7T</td>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>{'Format:': ' Kindle Edition'}</td>\n",
       "      <td>mb is one of the original western writers and ...</td>\n",
       "      <td>A Western</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>mb is one of the original western writers and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified  reviewTime      reviewerID        asin  \\\n",
       "0        4      True  2014-07-03  A2LSKD2H9U8N0J  B000FA5KK0   \n",
       "1        5      True  2014-05-26  A2QP13XTJND1QS  B000FA5KK0   \n",
       "2        5      True  2016-09-16   A8WQ7MAG3HFOZ  B000FA5KK0   \n",
       "3        5      True  2016-03-03   A1E0MODSRYP7O  B000FA5KK0   \n",
       "4        5      True  2015-09-10   AYUTCGVSM1H7T  B000FA5KK0   \n",
       "\n",
       "                            style  \\\n",
       "0  {'Format:': ' Kindle Edition'}   \n",
       "1  {'Format:': ' Kindle Edition'}   \n",
       "2  {'Format:': ' Kindle Edition'}   \n",
       "3  {'Format:': ' Kindle Edition'}   \n",
       "4  {'Format:': ' Kindle Edition'}   \n",
       "\n",
       "                                          reviewText            summary title  \\\n",
       "0  pretty good story, a little exaggerated, but i...  pretty good story   NaN   \n",
       "1  if you've read other max brand westerns, you k...   A very good book   NaN   \n",
       "2                       love max, always a fun twist         Five Stars   NaN   \n",
       "3                      as usual for him, a good book             a good   NaN   \n",
       "4  mb is one of the original western writers and ...          A Western   NaN   \n",
       "\n",
       "   day  month  year  sentiment  \\\n",
       "0    3      7  2014          2   \n",
       "1   26      5  2014          2   \n",
       "2   16      9  2016          2   \n",
       "3    3      3  2016          2   \n",
       "4   10      9  2015          2   \n",
       "\n",
       "                                        review_clean  \n",
       "0  pretty good story a little exaggerated but i l...  \n",
       "1  if youve read other max brand westerns you kno...  \n",
       "2                        love max always a fun twist  \n",
       "3                       as usual for him a good book  \n",
       "4  mb is one of the original western writers and ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Ternary Class to Binary Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My aim is to compare products and determine less seller products with giving importance to negative reviewed books to take action. So, to focus on less ratings, I will divide my target to two-class as positive and negative where 1 and 2 rating values counted as negative and others are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_two_sentiment(overall):\n",
    "    '''This function encodes the rating 1 and 2 as 0, others as 1'''\n",
    "    if overall >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['overall'].apply(calc_two_sentiment) #applying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2031419\n",
       "0     109546\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data is big enough for runing usual computers. I can use cloud but it also takes hours for each running. Even for 100000 rows, some models took more than 3 hours. If I try models for whole set, I have to wait more than half day for each change in model. So, I prefer to choose sample, find the best model with it and apply this model to whole dataset. For deep learning models, I prefer to take my sample unbalanced data as first 100000 row of the clean data. Because, deep learning model can handle unbalanced classes better than linear or gradient machine learning models. I would like to see how the model will do with unbalanced version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torch = df.head(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write codes easily and keep less data in memory, I will just only choose the columns which I need for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torch= df_torch.loc[:, ['review_clean', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use more easily I will divide train-test splits and write them csv files. Normally, I will split validation data from train and I will use this as unseen data to compare how the model does. But, firstly I will split my data to keep test set in computer. If i need to check small size data or same test future, I would like to keep test data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_torch, test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv', index = False) # my main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test.csv', index = False) # not for using now, for keeping just in case as small data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 80000\n",
      "Number of testing examples: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data to Torch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are two pages which gave me idea to get baseline and how to use FastText class. The code of the FastText class also is taken from these two people. I combined their work and changed according to my data and tried different parameters for models.\n",
    "\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis\n",
    "\n",
    "https://www.kaggle.com/lalwaniabhishek/abhishek-lalwani-bits-twitter-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important concepts of TorchText is the Field function, which defines how the data should be processed. \n",
    "\n",
    "I will use TEXT to define how the reviews will be processed and use TARGET field to process the target. As a preprocessing technique, I will use bi-grams. It creates a set of co-occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(text):\n",
    "    '''creating set of co-occuring words'''\n",
    "    bi_grams = set(zip(*[text[i:] for i in range(2)]))\n",
    "    for bi_gram in bi_grams:\n",
    "        text.append(' '.join(bi_gram))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'book', 'I love', 'this book', 'love this']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check bi-gram function is working proporly or not\n",
    "generate_bigrams(['I', 'love', 'this', 'book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My bi-gram function is working properly, I can see two-words couples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will define my model to preprocess with bi-grams, SpaCy tokenizer and LabelField to handle the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_bigrams)\n",
    "TARGET = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_train = [('review_clean', TEXT),('sentiment', TARGET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using TabularDataset, we will take our train, test splits easily each time and preprocessed with bi-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking training data from train.csv\n",
    "train_data = data.TabularDataset(path = 'train.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To check the first elements in train\n",
    "# print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to split a validation data from my train data, to make sure my model is doing good. I will use default for split sizes and define my random seed to get same data each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation set from train data\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56000\n",
      "Number of validation examples: 24000\n",
      "Number of testing examples: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to build a vocabulary. There are lots of words so I will define maximum top words sizes. Then, I will load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0519 16:21:52.618643 4541910464 vocab.py:431] Loading vectors from .vector_cache/glove.6B.100d.txt.pt\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only build vocabulary on train set. Because, in machine learning models test set must not be seen before to test it well. So I do not add validation set, because I want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique tokens in TEXT vocabulary: 25002\n",
      "# of unique tokens in TARGET vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"# of unique tokens in TARGET vocabulary: {len(TARGET.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose my max vocabulary size 25000, it means there is two additional tokens like <...> default. Because all sentences in the batches must be at same size. To make each sentence equal in the batch, it padded longer or shorter batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 237680), ('and', 149130), ('a', 139606), ('i', 135782), ('to', 130757), ('of', 98507), (' ', 88735), ('is', 78287), ('this', 76680), ('it', 73911), ('in', 65307), ('was', 60753), ('that', 56410), ('book', 51555), ('for', 43846), ('story', 39234), ('but', 37773), ('her', 37674), ('with', 37553), ('read', 35264), ('you', 34167), ('nt', 33702), ('\\n\\n', 32400), ('she', 28707), ('not', 28230)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(25)) # to see most common words in the vocabulary with their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I create my vocabulary using pre-trained embeddings. The final step of preparing data to Torch model is creating iterators. I will iterate train and evaluation loop and get a batch of examples which indexed and converted into tensors for each iteration. I will use Iterator function of torch. Also, I need to keep the tensors which returned by iterators in GPU so I will use torch.device function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set batch size and iterators for train and validation data \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.Iterator(dataset = train_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = True, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)\n",
    "valid_iterator = data.Iterator(dataset = valid_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ready classes to building a model. I prefer to use FastText class for baseline model, because gets comparable results significantly faster and using around half of the parameters. The details about this class can be found in [Bag of Tricks for Efficient Text Classification paper](https://arxiv.org/abs/1607.01759). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(0.5) # for adding dropout\n",
    "        \n",
    "    def forward(self, text):\n",
    "    \n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only has 2 layers that have any parameters, the linear and the embedding layer. There in no RNN layer. It will calculate the word embedding by using embedding layer, and taking average of them feeds the linear layer. Now, I will create my FastText class with defining dimensions and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab) #vocabulary size \n",
    "EMBEDDING_DIM = 100 # embedding dimension\n",
    "OUTPUT_DIM = 1 # our output has only 2 classes - 0/1. So, it is one-dimensional.\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # string to integer method on padding tokens\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare trainable parameters in different models, count parameters function will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will copy pre-trained vectors to my embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.6578,  0.9299,  0.0580,  ..., -0.9173,  1.2022,  0.2694],\n",
       "        [-0.3626,  0.1501,  1.4050,  ...,  0.0213,  0.3717, -0.6314],\n",
       "        [-1.3447, -1.4811,  0.7253,  ..., -0.5115, -0.9313, -0.3301]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must assign zero for initial weight for unknown and padding tokens. I have already defined padding token before as PAD_IDX. So, I will define unknows as UNK_IDX and set initials to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, firstly I will create optimizer and criterion. Optimizer updates parameters of module. I will use SGD and Adam as optimizer. SGD is a variant of gradient descent. It does not perform on whole dataset, it computes on a small subset or random selection. It performs good when the learning rate is low. Optimizer needs two parameters, one is optimizer type and second is learning rate. Adam optimizer is a technique which implementing adaptive learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried both optimizers one by one with uncommenting the cell below also with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will define loss function. My target contains binary labels, so I will choose binary loss function as criterion.\n",
    "Cross-entropy loss is commonly used for classification porblems. Also, BCEWithLogitsLoss is contains one sigmoid layer and binary cross-entropy loss. So, I will use this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# keeping model and criterion in GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss will be calculated by using criterion but I want to see accuracy to compare models. This function turn the values to 0-1 with rounding them in sigmoid layer. Then, it calculates the rounded predictions equal actual labels and take the mean of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(pred, target):\n",
    "      \n",
    "    # rounding predictions to the closest integer\n",
    "    rounded_pred = torch.round(torch.sigmoid(pred))\n",
    "    true = (rounded_pred == target).float() # convert into float for taking mean \n",
    "    accuracy = true.sum() / len(true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the train method\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0 # \n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator: # for each batch\n",
    "        \n",
    "        optimizer.zero_grad() # zero gradient\n",
    "        # PyTorch does not automatically zero the gradients calculated from the last gradient calculation\n",
    "        \n",
    "        predictions = model(batch.review_clean).squeeze(1) # with feeding batch with reviews no need to .forward\n",
    "        \n",
    "        #squeeze for removing dimension in the list and taking only batch size \n",
    "        #bec. torch wants predictions input as batch size\n",
    "        \n",
    "        loss = criterion(predictions, batch.sentiment) # calculating loss\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.sentiment) # calculating accuracy with taking mean\n",
    "        \n",
    "        loss.backward() #gradient of each parameter\n",
    "        \n",
    "        optimizer.step() #update the optimizer algorithm\n",
    "        \n",
    "        # loss and accuracy by epoches\n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) # returning loss and acc avg across epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do same function for evaluate validation part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    '''Evaluating validation set'''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.review_clean).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.sentiment)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.sentiment)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also use a function which informs that how long each epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model for Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried for different epoch numbers and from result I prefer to choose 5, because it keeps the information mainly in first 5 epoches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 8s\n",
      "\tTrain Loss: 0.454 | Train Acc: 88.82%\n",
      "\t Val. Loss: 0.717 |  Val. Acc: 91.69%\n",
      "Epoch: 02 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 0.318 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.446 |  Val. Acc: 91.62%\n",
      "Epoch: 03 | Epoch Time: 2m 25s\n",
      "\tTrain Loss: 0.236 | Train Acc: 91.84%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 92.45%\n",
      "Epoch: 04 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.194 | Train Acc: 92.48%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 92.58%\n",
      "Epoch: 05 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.171 | Train Acc: 93.04%\n",
      "\t Val. Loss: 0.665 |  Val. Acc: 92.56%\n"
     ]
    }
   ],
   "source": [
    "# with Adam optimizer\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks overfit, so I added dropout and run again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.427 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.641 |  Val. Acc: 91.71%\n",
      "Epoch: 02 | Epoch Time: 2m 33s\n",
      "\tTrain Loss: 0.302 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.404 |  Val. Acc: 91.78%\n",
      "Epoch: 03 | Epoch Time: 2m 26s\n",
      "\tTrain Loss: 0.227 | Train Acc: 91.94%\n",
      "\t Val. Loss: 0.441 |  Val. Acc: 92.55%\n",
      "Epoch: 04 | Epoch Time: 2m 28s\n",
      "\tTrain Loss: 0.189 | Train Acc: 92.58%\n",
      "\t Val. Loss: 0.568 |  Val. Acc: 92.31%\n",
      "Epoch: 05 | Epoch Time: 2m 27s\n",
      "\tTrain Loss: 0.168 | Train Acc: 93.16%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 92.38%\n"
     ]
    }
   ],
   "source": [
    "# with Adam optimizer with dropout\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still overfit, so I changed learning rate and tried again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer with Different Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run the code with different learning rates to see which one gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.157 | Train Acc: 93.55%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 92.24%\n",
      "Epoch: 02 | Epoch Time: 2m 36s\n",
      "\tTrain Loss: 0.156 | Train Acc: 93.56%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 92.22%\n",
      "Epoch: 03 | Epoch Time: 2m 32s\n",
      "\tTrain Loss: 0.154 | Train Acc: 93.71%\n",
      "\t Val. Loss: 0.737 |  Val. Acc: 92.21%\n",
      "Epoch: 04 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.153 | Train Acc: 93.77%\n",
      "\t Val. Loss: 0.747 |  Val. Acc: 92.29%\n",
      "Epoch: 05 | Epoch Time: 2m 25s\n",
      "\tTrain Loss: 0.152 | Train Acc: 93.75%\n",
      "\t Val. Loss: 0.767 |  Val. Acc: 92.20%\n"
     ]
    }
   ],
   "source": [
    "# with Adam optimizer with dropout lr e-4\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting problem is still available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will change my optimizer and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 45s\n",
      "\tTrain Loss: 0.647 | Train Acc: 76.40%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 91.68%\n",
      "Epoch: 02 | Epoch Time: 2m 44s\n",
      "\tTrain Loss: 0.531 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 91.68%\n",
      "Epoch: 03 | Epoch Time: 2m 32s\n",
      "\tTrain Loss: 0.459 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.355 |  Val. Acc: 91.68%\n",
      "Epoch: 04 | Epoch Time: 2m 20s\n",
      "\tTrain Loss: 0.412 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 91.68%\n",
      "Epoch: 05 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.380 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.305 |  Val. Acc: 91.68%\n"
     ]
    }
   ],
   "source": [
    "# with SGD optimizer - lr e-3\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is less than Adam optimizer but, these results are better for loss and overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 14s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.81%\n",
      "\t Val. Loss: 0.767 |  Val. Acc: 92.21%\n",
      "Epoch: 02 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.80%\n",
      "\t Val. Loss: 0.767 |  Val. Acc: 92.21%\n",
      "Epoch: 03 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.86%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 04 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.85%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 05 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.89%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n"
     ]
    }
   ],
   "source": [
    "# with SGD optimizer with dropout\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is overfitting again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Optimizer with Different Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 59s\n",
      "\tTrain Loss: 0.150 | Train Acc: 93.80%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 02 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.82%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 03 | Epoch Time: 2m 19s\n",
      "\tTrain Loss: 0.150 | Train Acc: 93.88%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 04 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.87%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n",
      "Epoch: 05 | Epoch Time: 2m 8s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.81%\n",
      "\t Val. Loss: 0.766 |  Val. Acc: 92.22%\n"
     ]
    }
   ],
   "source": [
    "# with SGD optimizer with dropout different lr \n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is higher but validation loss is higher also, so I decided to run with different parameters again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Optimizer without Dropout Layer with Smaller Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 9s\n",
      "\tTrain Loss: 0.703 | Train Acc: 9.90%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 30.88%\n",
      "Epoch: 02 | Epoch Time: 2m 32s\n",
      "\tTrain Loss: 0.686 | Train Acc: 85.03%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 76.10%\n",
      "Epoch: 03 | Epoch Time: 2m 57s\n",
      "\tTrain Loss: 0.670 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.655 |  Val. Acc: 88.40%\n",
      "Epoch: 04 | Epoch Time: 3m 7s\n",
      "\tTrain Loss: 0.655 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.633 |  Val. Acc: 90.53%\n",
      "Epoch: 05 | Epoch Time: 2m 35s\n",
      "\tTrain Loss: 0.640 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 91.02%\n"
     ]
    }
   ],
   "source": [
    "# with SGD optimizer without dropout lr e-4\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found best results with SGD optimizer and learning rate as e-3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Tri-Gram Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I would like to see what it will change if I will group my words as three instead of two. I will do some steps again because my torch ready data will change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(text):\n",
    "    '''creating set of 3 co-occuring words'''\n",
    "    tri_grams = set(zip(*[text[i:] for i in range(3)]))\n",
    "    for tri_gram in tri_grams:\n",
    "        text.append(' '.join(tri_gram))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'book', 'love this book', 'I love this']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check tri-gram function is working proporly or not\n",
    "generate_trigrams(['I', 'love', 'this', 'book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_trigrams)\n",
    "TARGET = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_train = [('review_clean', TEXT),('sentiment', TARGET)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking training data from train.csv\n",
    "train_data = data.TabularDataset(path = 'train.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vars(train_data[0])) # to check tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation set from train data\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.Iterator(dataset = train_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = True, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)\n",
    "valid_iterator = data.Iterator(dataset = valid_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab) #vocabulary size \n",
    "EMBEDDING_DIM = 100 # embedding dimension\n",
    "OUTPUT_DIM = 1 # our output has only 2 classes - 0/1. So, it is one-dimensional.\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # string to integer method on padding tokens\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 1.5221, -0.3108, -0.2902,  ..., -0.2051, -0.9059, -0.8559],\n",
       "        [ 0.9666, -0.3822, -0.2585,  ..., -1.0574, -0.6668,  0.1646],\n",
       "        [ 1.8935, -0.8303,  0.2935,  ..., -0.6399, -1.8376, -1.9168]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# keeping model and criterion in GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 39s\n",
      "\tTrain Loss: 0.443 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 91.69%\n",
      "Epoch: 02 | Epoch Time: 2m 28s\n",
      "\tTrain Loss: 0.324 | Train Acc: 91.66%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 91.63%\n",
      "Epoch: 03 | Epoch Time: 2m 31s\n",
      "\tTrain Loss: 0.248 | Train Acc: 91.74%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 92.18%\n",
      "Epoch: 04 | Epoch Time: 2m 46s\n",
      "\tTrain Loss: 0.206 | Train Acc: 92.15%\n",
      "\t Val. Loss: 0.539 |  Val. Acc: 92.41%\n",
      "Epoch: 05 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.182 | Train Acc: 92.63%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 92.34%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 15s\n",
      "\tTrain Loss: 0.221 | Train Acc: 92.00%\n",
      "\t Val. Loss: 0.410 |  Val. Acc: 92.22%\n",
      "Epoch: 02 | Epoch Time: 2m 29s\n",
      "\tTrain Loss: 0.221 | Train Acc: 91.98%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 92.28%\n",
      "Epoch: 03 | Epoch Time: 2m 15s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.97%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 92.27%\n",
      "Epoch: 04 | Epoch Time: 2m 33s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.96%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 92.27%\n",
      "Epoch: 05 | Epoch Time: 2m 34s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.95%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 92.27%\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better than Adam optimizer but not better than bi-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I insert the code below for if someone want to see how to try downloaded model for different unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.402 | Test Acc: 92.25%\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Results : \n",
    " - with SGD optimizer and learning rate e-3, 91.66% accuracy for train and 91.68% accuracy for validation obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Augmentation To Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better results, I will try to add augmentation to my data.\n",
    "\n",
    "Further information can be found in https://github.com/makcedward/nlpaug/blob/master/example/textual_augmenter.ipynb.\n",
    "\n",
    "I only try one method to see synonym augmentation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ezgi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ezgi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "I like this book\n",
      "Augmented Text:\n",
      "I care this book\n"
     ]
    }
   ],
   "source": [
    "# to see differences in normal and augmented texts\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "text = 'I like this book'\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(text):\n",
    "    '''this function changes texts according to synonym augmentation'''\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_clean'], inplace=True) #checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = df.head(100000) #takind first 100000 as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug= df_aug.loc[:, ['review_clean', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug, test_aug = train_test_split(df_aug, test_size=0.2,random_state = 42) #splitting for using same part as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezgi/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75241</td>\n",
       "      <td>new to me author but he spins a good tale i en...</td>\n",
       "      <td>2</td>\n",
       "      <td>new to me author but he spin around a serious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48970</td>\n",
       "      <td>had to keep reading addicted must know what ha...</td>\n",
       "      <td>2</td>\n",
       "      <td>get to keep reading hook mustiness know what h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44979</td>\n",
       "      <td>good book that will be enjoyed by inmates for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>good holy scripture that will live enjoyed by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13571</td>\n",
       "      <td>this book was a nice little slightly erotic re...</td>\n",
       "      <td>2</td>\n",
       "      <td>this book was a gracious little somewhat eroti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92751</td>\n",
       "      <td>ive never read a work by mary jane clark befor...</td>\n",
       "      <td>1</td>\n",
       "      <td>ive never read a work by mary jane clark befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_clean  sentiment  \\\n",
       "75241  new to me author but he spins a good tale i en...          2   \n",
       "48970  had to keep reading addicted must know what ha...          2   \n",
       "44979  good book that will be enjoyed by inmates for ...          2   \n",
       "13571  this book was a nice little slightly erotic re...          2   \n",
       "92751  ive never read a work by mary jane clark befor...          1   \n",
       "\n",
       "                                              review_emb  \n",
       "75241  new to me author but he spin around a serious ...  \n",
       "48970  get to keep reading hook mustiness know what h...  \n",
       "44979  good holy scripture that will live enjoyed by ...  \n",
       "13571  this book was a gracious little somewhat eroti...  \n",
       "92751  ive never read a work by mary jane clark befor...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aug['review_emb'] = train_aug['review_clean'].apply(lambda x: embedding(x))\n",
    "train_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug.to_csv('train_aug.csv', index = False) # to keep augmented dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reviews were changed according to synonym. This wss the first step to figure out how data augmentation works. When we put this data to torch model, it will not give good results because it needs more deeper work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Improvements for This Notebook\n",
    "\n",
    "After talking our instructor Bryan Arnold, some steps were determined to improve more this model;\n",
    "\n",
    "- Tri-grams and bi-grams applied to model seperately, new dictionary can be formed which contains both of them.\n",
    "- Data augmentation will be added to data.\n",
    "- Dropout layer and other layers can be changed or new layers can be added.\n",
    "- Test-time augmentation can be added. \n",
    "- I have already run the model for different learning rates but more different values can be tried. \n",
    "- I will run the model for higher epoch numbers. Each epoch takes time to I only tried for 5, it can be increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will continue to try other deep learning models to find better results and to find easily tuned models My next step is to work on Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
