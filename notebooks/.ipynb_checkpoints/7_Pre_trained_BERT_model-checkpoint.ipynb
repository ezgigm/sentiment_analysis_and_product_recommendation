{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXWbEMlvhAtS"
   },
   "source": [
    "# Modeling with Pre-trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvD2wq8HhAtT"
   },
   "source": [
    "In this notebook, my aim is to try pre-trained model (BERT) to figure out how results will change when I use pre-trained model. \n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a model which released in late 2018 for transfer learning in NLP. Advantages of pre-trained BERT;\n",
    "\n",
    "- Model already has a lot of information so less time needs to tune model.\n",
    "- It is already pre-trained so less data is needed to train the model.\n",
    "- Because of simple fine-tuning procedure, it generally gives better results.\n",
    "\n",
    "To get my results more quickly, I used Google Colab in this model with offered free GPU. \n",
    "\n",
    "To learn how to use BERT, I used the tutorial below,\n",
    "\n",
    "http://mccormickml.com/2019/07/22/BERT-fine-tuning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "brprw0cBT-2C",
    "outputId": "cec944de-154f-4e13-f9bd-63a819e6abc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# find the exact GPU name and assign it to this notebook\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# To check the device name is found like below\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IAZGL6_fUFNq",
    "outputId": "a02786d6-9526-4f26-a93a-39ebcc91b5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If GPU is available setting to use this\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Setting the PyTorch to use GPU as memory\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# To check is GPU is not found\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfZE4yyihAte"
   },
   "source": [
    "Now, I have to download transformers package because BERT can be used with it. Transformers library contains PyTorch and Tensorflow implementations, pre-trained model weights, usage scripts and conversion utilities for more than 10 different models, details can be found [here](https://huggingface.co/transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "-N6J8qsxUyKO",
    "outputId": "f2501329-2c4a-4517-f3e8-e06ec24035e4"
   },
   "outputs": [],
   "source": [
    "#  !pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AMHulqdGhAtj",
    "outputId": "c629ef95-5e7f-4cab-8cd6-eee4c62c8cfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# to use pandas dataframe\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# for modeling and evaluation\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# to plot\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU_reKedhAtm"
   },
   "source": [
    "To download my data to Google Colab, I need to connect Colab with my Drive. So, I used the cell below and put the confirmation code to this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "jVbeyyADWb9E",
    "outputId": "76b41a65-9c73-4074-8613-9766b4c114c6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrgHKJkuViFo"
   },
   "outputs": [],
   "source": [
    "# taking data to df from Google Drive\n",
    "path = '/content/drive/My Drive/train/train.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cDaK2F8IXob8",
    "outputId": "c9bd8c0f-cdd2-4fbf-abcc-277283e438e7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_clean    0\n",
       "sentiment       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() # to check the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1wXnrg8hzNz"
   },
   "outputs": [],
   "source": [
    "# df.dropna(inplace=True) # if null is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHWPl46ChAtx"
   },
   "source": [
    "Now, I will assign my text and target values to lists to prepare them to model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSYeOlrUX74v"
   },
   "outputs": [],
   "source": [
    "text = df['review_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_TCmxQIYUV2"
   },
   "outputs": [],
   "source": [
    "target = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff9IRAKshAt2"
   },
   "source": [
    "I have to split my text to tokens to feed it to BERT. I also set lower case True, just in case because I have already preprocessed lower-case before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "1221df09c2cd4c969db6546b76504886",
      "ce69c8b1cbbf4ced800784099cd54c3e",
      "6ed090fbe52c47f2b3038d48b05f9d0f",
      "4388cda8b677438f8e89e1c9c4a386b7",
      "ed9f67484fe747b0bab16c18b34e69b9",
      "1b4bbefd5982432fbf1cbe5b62ef6046",
      "178db16194c04743a79c3c4144b658e5",
      "2d1b451e4d4d46d78a463e18b9c87ae3"
     ]
    },
    "colab_type": "code",
    "id": "mzSyvFA5YYZp",
    "outputId": "e28c49fd-778b-447d-9e1a-fac7847a3301"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1221df09c2cd4c969db6546b76504886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# using Bert-Tokenizer to tokenize text \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qmhhE-phAt5"
   },
   "source": [
    "Now, I will try tokenizer firstly to first text of my data to check and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "UAytA7woYd-F",
    "outputId": "8d81eefc-64e5-41fa-aea2-2a7414c3352a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  this is a cute little book that is fairly short and easy to read this author fell in love with a silver camper bought it refurbished it and off she went to camp for six months those six months turned into a trip of a life time and a move across the country by the time it was over\n",
      "\n",
      "i give it a little bit lower score than others because i didnt find it laugh out loud hilarious it was funny in a cute way and for someone who owns a camper and goes camping often much of this book is right on target about how not to  and it was right on target about life in a campground\n",
      "\n",
      "overall this is an enjoyable book it is cute and clever it is not a long read and it is right on target in many ways \n",
      "enjoy\n",
      "Tokenized:  ['this', 'is', 'a', 'cute', 'little', 'book', 'that', 'is', 'fairly', 'short', 'and', 'easy', 'to', 'read', 'this', 'author', 'fell', 'in', 'love', 'with', 'a', 'silver', 'camp', '##er', 'bought', 'it', 'refurbished', 'it', 'and', 'off', 'she', 'went', 'to', 'camp', 'for', 'six', 'months', 'those', 'six', 'months', 'turned', 'into', 'a', 'trip', 'of', 'a', 'life', 'time', 'and', 'a', 'move', 'across', 'the', 'country', 'by', 'the', 'time', 'it', 'was', 'over', 'i', 'give', 'it', 'a', 'little', 'bit', 'lower', 'score', 'than', 'others', 'because', 'i', 'didn', '##t', 'find', 'it', 'laugh', 'out', 'loud', 'hilarious', 'it', 'was', 'funny', 'in', 'a', 'cute', 'way', 'and', 'for', 'someone', 'who', 'owns', 'a', 'camp', '##er', 'and', 'goes', 'camping', 'often', 'much', 'of', 'this', 'book', 'is', 'right', 'on', 'target', 'about', 'how', 'not', 'to', 'and', 'it', 'was', 'right', 'on', 'target', 'about', 'life', 'in', 'a', 'campground', 'overall', 'this', 'is', 'an', 'enjoyable', 'book', 'it', 'is', 'cute', 'and', 'clever', 'it', 'is', 'not', 'a', 'long', 'read', 'and', 'it', 'is', 'right', 'on', 'target', 'in', 'many', 'ways', 'enjoy']\n",
      "Token IDs:  [2023, 2003, 1037, 10140, 2210, 2338, 2008, 2003, 7199, 2460, 1998, 3733, 2000, 3191, 2023, 3166, 3062, 1999, 2293, 2007, 1037, 3165, 3409, 2121, 4149, 2009, 18662, 2009, 1998, 2125, 2016, 2253, 2000, 3409, 2005, 2416, 2706, 2216, 2416, 2706, 2357, 2046, 1037, 4440, 1997, 1037, 2166, 2051, 1998, 1037, 2693, 2408, 1996, 2406, 2011, 1996, 2051, 2009, 2001, 2058, 1045, 2507, 2009, 1037, 2210, 2978, 2896, 3556, 2084, 2500, 2138, 1045, 2134, 2102, 2424, 2009, 4756, 2041, 5189, 26316, 2009, 2001, 6057, 1999, 1037, 10140, 2126, 1998, 2005, 2619, 2040, 8617, 1037, 3409, 2121, 1998, 3632, 13215, 2411, 2172, 1997, 2023, 2338, 2003, 2157, 2006, 4539, 2055, 2129, 2025, 2000, 1998, 2009, 2001, 2157, 2006, 4539, 2055, 2166, 1999, 1037, 29144, 3452, 2023, 2003, 2019, 22249, 2338, 2009, 2003, 10140, 1998, 12266, 2009, 2003, 2025, 1037, 2146, 3191, 1998, 2009, 2003, 2157, 2006, 4539, 1999, 2116, 3971, 5959]\n"
     ]
    }
   ],
   "source": [
    "# to see the original text\n",
    "print(' Original: ', text[0])\n",
    "\n",
    "# to see the tokenized text\n",
    "print('Tokenized: ', tokenizer.tokenize(text[0]))\n",
    "\n",
    "# to see the token ids of text\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ht02twZ0hAt8"
   },
   "source": [
    "Now, I am sure my tokenizer is working and I will apply it to whole rows. In addition to this, BERT needs special formatting such as it needs special tokens at the end of the each text and at the begining of the each text. To prepare my text to BERT, I will do some preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syEQwPm-hAt9"
   },
   "source": [
    "# Pre-processing Text for BERT Formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AhjjLAwhAt9"
   },
   "source": [
    "In this step;\n",
    "\n",
    "- I will add special tokens to the begining and at the end of each text. At the end of the each text, I have to add [SEP] token. Also, [CLS] token will be added to begining of the each sentence for classification problems. \n",
    "- I will set all texts to a single constant length. For BERT, all texts must be padded to fixed lenght and it can be 512 token maximum. Padding is doing with [PAD] token, which is indexed as 0 in BERT vocabulary. If the tokens of the text is less than maximum token number [PAD] token assigns after the [SEP] token until reaching maximum token number. Also, attention mask value for these [PAD] tokens are 0. Because, if the token is padding, attention mask is 1, if not it is 0. In this notebook, I tried both 512 and 128 max lenght tokens. Training and evaluations of 512 took 2 times more than 128 and the results were not significantly different. So, I advice to set 128, if somebody wants to run this notebook with same data. The handycap of setting max length to 512 is that for my data, I also have some texts which are longer than 512, but when I checked their number, I found them not too much. So I assume that if I set the lenght to 512, I will not lose too much information. This is one of the weekest points of BERT, it allows maximum 512 tokens.\n",
    "- I will map tokens to their corresponding ids.\n",
    "\n",
    "To begin these steps, I will use .encode tool of BertTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "9Vm0MhHKYkIa",
    "outputId": "67c776bf-c022-40c1-c4cb-e010994f6dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  this is a cute little book that is fairly short and easy to read this author fell in love with a silver camper bought it refurbished it and off she went to camp for six months those six months turned into a trip of a life time and a move across the country by the time it was over\n",
      "\n",
      "i give it a little bit lower score than others because i didnt find it laugh out loud hilarious it was funny in a cute way and for someone who owns a camper and goes camping often much of this book is right on target about how not to  and it was right on target about life in a campground\n",
      "\n",
      "overall this is an enjoyable book it is cute and clever it is not a long read and it is right on target in many ways \n",
      "enjoy\n",
      "Token IDs: [101, 2023, 2003, 1037, 10140, 2210, 2338, 2008, 2003, 7199, 2460, 1998, 3733, 2000, 3191, 2023, 3166, 3062, 1999, 2293, 2007, 1037, 3165, 3409, 2121, 4149, 2009, 18662, 2009, 1998, 2125, 2016, 2253, 2000, 3409, 2005, 2416, 2706, 2216, 2416, 2706, 2357, 2046, 1037, 4440, 1997, 1037, 2166, 2051, 1998, 1037, 2693, 2408, 1996, 2406, 2011, 1996, 2051, 2009, 2001, 2058, 1045, 2507, 2009, 1037, 2210, 2978, 2896, 3556, 2084, 2500, 2138, 1045, 2134, 2102, 2424, 2009, 4756, 2041, 5189, 26316, 2009, 2001, 6057, 1999, 1037, 10140, 2126, 1998, 2005, 2619, 2040, 8617, 1037, 3409, 2121, 1998, 3632, 13215, 2411, 2172, 1997, 2023, 2338, 2003, 2157, 2006, 4539, 2055, 2129, 2025, 2000, 1998, 2009, 2001, 2157, 2006, 4539, 2055, 2166, 1999, 1037, 29144, 3452, 2023, 2003, 2019, 102]\n"
     ]
    }
   ],
   "source": [
    "# To tokenize texts and map the tokens to corresponding IDs\n",
    "input_ids = [] # creating empty list to keep input ids\n",
    "\n",
    "# Loop for each element in text\n",
    "for element in text:\n",
    "\n",
    "    encoded_text = tokenizer.encode(element,add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                                         max_length = 128)  # setting all sentences to max_len\n",
    "                                \n",
    "    # firstly I tried it for 512 and I tried for 128, last trial belongs to 128 so values are for 128\n",
    "    # there were no significant difference between 128 or 512 \n",
    "    \n",
    "    # keeping encoded texts as list\n",
    "    input_ids.append(encoded_text)\n",
    "\n",
    "# printing to check\n",
    "print('Original: ', text[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjnH0kL3hAuA"
   },
   "source": [
    "For padding part, I will use pad_sequences function from keras.preprocessing.sequence library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Cp9BPRd1tMIo",
    "outputId": "473ddeb0-569f-477f-e2f6-54098f69bff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 128 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "max_length = 128 # for 512, I set it to 512\n",
    "\n",
    "print(f'\\nPadding/truncating all sentences to {max_length} values...')\n",
    "\n",
    "print(f'\\nPadding token: \"{tokenizer.pad_token}\", ID: {tokenizer.pad_token_id}')\n",
    "\n",
    "# pad sequence function for pad tokens and assign zeros for shorter than max lenght\n",
    "input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ojtKhuPatXT"
   },
   "outputs": [],
   "source": [
    "# To keep attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Loop for each element in inputs\n",
    "for element in input_ids:\n",
    "    \n",
    "    #   I assigned the empty tokens to 0 in above cell so if token is 0, setting the mask zero\n",
    "    #   If token is larger than zero it means it is real token ID and set it to 1\n",
    "    att_mask = [int(token_id > 0) for token_id in element]\n",
    "    \n",
    "    # keeping attention masks\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jheo34PRhAuG"
   },
   "source": [
    "Now, I have to split my data for validation. I choose same random state and test size to compare my results with previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F92-y3sJawbO"
   },
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, target, \n",
    "                                                            random_state=42, test_size=0.2)\n",
    "# Splitting masks also with same parameters\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, target,\n",
    "                                             random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aE2LGnDEhAuN"
   },
   "source": [
    "I will concert all inputs and targets to torch tensors, it is required datatype for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdQSuZ94a7f2"
   },
   "outputs": [],
   "source": [
    "# for inputs\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "# for target values\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "# for masks\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m5nE_wKbSCA"
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # in tutorial it is recommended that to set 16 or 32. I choose 32 for more batches\n",
    "\n",
    "# creating dataloader for train set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# creating dataloader for validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDJcTsahhAuS"
   },
   "source": [
    "# Pre-trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hz6vz6SAhAuT"
   },
   "source": [
    "I have formatted data for BERT, so I wil begin fine tuning of BERT. BERT has many classes for fine tuning, such as BertModel, BertForNextSentencePrediction etc. In this notebook, I will use BertForSequenceClassification. This is the simple BERT model with an added single layer on top for classification. There are different pre-trained BERT models. “bert-base-uncased” is the version that has only lowercase letters (“uncased”) and is the smaller version of the two (“base” vs “large”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "abbc04b4375e44a29d4e4b72e0a17ddc",
      "4ae10cc6fe9a46a88c52ad56a93f80be",
      "6a309c31776c490cba4e1a6b50a00310",
      "ef904c4376514ea19c43d9c5bf00fc48",
      "d41bf1f7e9c84326a417a039ba18b051",
      "d21cffd4fe2f4b23b33660171258f1b3",
      "a87b027c20344c588c9f24797b6c5196",
      "71e8d4bceb874a6393ded5c0dd1bc623",
      "231c99254ea64e2cbf7a301b42f48da8",
      "218e8495c5374aeb944dcc4be1676b08",
      "a65cbda0a4264b9bb05fbfe68dbc1b50",
      "210c1900677048aca380294fc94182f7",
      "46a011e796e241c6807b48cdfed0ccd1",
      "ee5121d2e2aa44fd83be75842ce1d288",
      "660cfc5586714404b7c0dfe74f05b5e9",
      "c86995273de742f29382438b10f1868b"
     ]
    },
    "colab_type": "code",
    "id": "KwAKvngYbYSB",
    "outputId": "be123fe1-7244-4158-dacc-eb4c1d02c45d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbc04b4375e44a29d4e4b72e0a17ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231c99254ea64e2cbf7a301b42f48da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # use base with lower case\n",
    "    num_labels = 2, # my target is binary so I choose 2   \n",
    "    output_attentions = False, # model does not return attentions weights\n",
    "    output_hidden_states = False) # for returning all hidden-states\n",
    "\n",
    "# inform Pytorch to run model in GPU\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxEvSgjdhAuW"
   },
   "source": [
    "To understand parameters and what embedding layer, transformers and output layer contains, I will print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "Oh4ApNSjbcAS",
    "outputId": "67dee7d3-adba-48dc-869d-2ea71ab798db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gJWEF4GhAuZ"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fixiN366hAuZ"
   },
   "source": [
    "To tune the optimizer, it is recommended in tutorial to choose learning rate for AdamW optimizer 5e-5, 3e-5 or 2e-5. I prefer 2-e5 to be more precise. 1e-8 is also very small for epsilon value but it is good to prevent any division by zero in the implementation, detailed [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KekfSfNPbnR_"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPUUU-AnbuPd"
   },
   "outputs": [],
   "source": [
    "# 5 epoches is too much for this model, but to get better results and make sure model is learning or not, I choose it\n",
    "epochs = 5\n",
    "\n",
    "# number of steps data loader times epoch number\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# setting scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZM2FxwESb0Ru"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(pred, true):\n",
    "    '''function for flatten accuracy'''\n",
    "    pred_flat = np.argmax(pred, axis=1).flatten()\n",
    "    labels_flat = true.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9EkYZgHb8nH"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    to see time taking second and returns time as string with hour, min, sec\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rX2Qzc_7ws73"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTqX1oGPhAup"
   },
   "source": [
    "My training cell generally consists of two parts. First one for training, second one for evaluation from validation data. Training part is unpacking my data, loading data to GPU, clear gradients which calculated in the previous pass. As a default, gradients are kept in PyTorch, so I will clear out them explicitly. There are forward and backward passes also in training loop because forward is needed for feeding input data through network and backward for backpropagation. And last steps for training part is optimizer and observe variables for monitoring progress.\n",
    "\n",
    "For evaluation part, data is unpacked again, it is loaded GPU like train part. There is forward pass to feed input data through network and calculating progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6Gra5nFriTDM",
    "outputId": "3307fe4f-10a5-4b96-a1f6-c2b5faa4dcd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:03:00.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:15.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:30.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:45.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:04:00.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:15.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:30.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:45.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:05:00.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:15.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:30.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:45.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:06:00.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:15.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:30.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:45.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:07:00.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:15.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:30.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:45.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:08:00.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:15.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:30.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:45.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:09:00.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:15.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:30.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:45.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:10:00.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:15.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:30.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:45.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:11:00.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:15.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:30.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:45.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:12:00.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:15.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epcoh took: 0:12:30\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.12\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:45.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:14.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:29.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:44.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:59.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:44.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:59.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:14.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:29.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:44.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:59.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:13.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:28.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:43.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:58.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:13.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:28.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:43.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:58.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:13.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:28.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:43.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:58.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:13.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:28.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:43.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:58.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:13.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:12:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.15\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:45.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:02:00.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:15.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:30.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:44.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:14.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:29.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:44.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:59.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:14.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:29.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:43.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:58.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:13.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:28.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:43.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:58.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:13.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:28.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:43.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:58.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:13.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:28.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:43.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:58.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:12.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:27.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:42.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:57.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:12.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:27.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:42.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:57.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:12.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epcoh took: 0:12:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:30.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:44.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:01:59.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:14.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:29.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:44.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:59.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:14.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:29.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:44.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:59.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:13.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:28.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:43.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:58.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:13.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:28.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:43.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:58.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:13.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:28.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:43.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:58.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:12.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:27.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:42.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:57.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:12.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:27.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:42.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:57.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:12.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:27.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:42.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:57.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:11.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:26.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:41.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:56.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:11.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:12:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.25\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of  2,000.    Elapsed: 0:00:15.\n",
      "  Batch    80  of  2,000.    Elapsed: 0:00:30.\n",
      "  Batch   120  of  2,000.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  2,000.    Elapsed: 0:01:00.\n",
      "  Batch   200  of  2,000.    Elapsed: 0:01:15.\n",
      "  Batch   240  of  2,000.    Elapsed: 0:01:29.\n",
      "  Batch   280  of  2,000.    Elapsed: 0:01:44.\n",
      "  Batch   320  of  2,000.    Elapsed: 0:01:59.\n",
      "  Batch   360  of  2,000.    Elapsed: 0:02:14.\n",
      "  Batch   400  of  2,000.    Elapsed: 0:02:29.\n",
      "  Batch   440  of  2,000.    Elapsed: 0:02:44.\n",
      "  Batch   480  of  2,000.    Elapsed: 0:02:59.\n",
      "  Batch   520  of  2,000.    Elapsed: 0:03:14.\n",
      "  Batch   560  of  2,000.    Elapsed: 0:03:29.\n",
      "  Batch   600  of  2,000.    Elapsed: 0:03:44.\n",
      "  Batch   640  of  2,000.    Elapsed: 0:03:58.\n",
      "  Batch   680  of  2,000.    Elapsed: 0:04:13.\n",
      "  Batch   720  of  2,000.    Elapsed: 0:04:28.\n",
      "  Batch   760  of  2,000.    Elapsed: 0:04:43.\n",
      "  Batch   800  of  2,000.    Elapsed: 0:04:58.\n",
      "  Batch   840  of  2,000.    Elapsed: 0:05:13.\n",
      "  Batch   880  of  2,000.    Elapsed: 0:05:28.\n",
      "  Batch   920  of  2,000.    Elapsed: 0:05:43.\n",
      "  Batch   960  of  2,000.    Elapsed: 0:05:58.\n",
      "  Batch 1,000  of  2,000.    Elapsed: 0:06:13.\n",
      "  Batch 1,040  of  2,000.    Elapsed: 0:06:27.\n",
      "  Batch 1,080  of  2,000.    Elapsed: 0:06:42.\n",
      "  Batch 1,120  of  2,000.    Elapsed: 0:06:57.\n",
      "  Batch 1,160  of  2,000.    Elapsed: 0:07:12.\n",
      "  Batch 1,200  of  2,000.    Elapsed: 0:07:27.\n",
      "  Batch 1,240  of  2,000.    Elapsed: 0:07:42.\n",
      "  Batch 1,280  of  2,000.    Elapsed: 0:07:57.\n",
      "  Batch 1,320  of  2,000.    Elapsed: 0:08:12.\n",
      "  Batch 1,360  of  2,000.    Elapsed: 0:08:27.\n",
      "  Batch 1,400  of  2,000.    Elapsed: 0:08:42.\n",
      "  Batch 1,440  of  2,000.    Elapsed: 0:08:56.\n",
      "  Batch 1,480  of  2,000.    Elapsed: 0:09:11.\n",
      "  Batch 1,520  of  2,000.    Elapsed: 0:09:26.\n",
      "  Batch 1,560  of  2,000.    Elapsed: 0:09:41.\n",
      "  Batch 1,600  of  2,000.    Elapsed: 0:09:56.\n",
      "  Batch 1,640  of  2,000.    Elapsed: 0:10:11.\n",
      "  Batch 1,680  of  2,000.    Elapsed: 0:10:26.\n",
      "  Batch 1,720  of  2,000.    Elapsed: 0:10:41.\n",
      "  Batch 1,760  of  2,000.    Elapsed: 0:10:56.\n",
      "  Batch 1,800  of  2,000.    Elapsed: 0:11:11.\n",
      "  Batch 1,840  of  2,000.    Elapsed: 0:11:26.\n",
      "  Batch 1,880  of  2,000.    Elapsed: 0:11:41.\n",
      "  Batch 1,920  of  2,000.    Elapsed: 0:11:56.\n",
      "  Batch 1,960  of  2,000.    Elapsed: 0:12:10.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:12:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.28\n",
      "  Validation took: 0:00:58\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:07:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# seed value for reproducibility\n",
    "seed_no = 42\n",
    "\n",
    "random.seed(seed_no)\n",
    "np.random.seed(seed_no)\n",
    "torch.manual_seed(seed_no)\n",
    "torch.cuda.manual_seed_all(seed_no)\n",
    "\n",
    "# Keeping accuracy, loss and timing values\n",
    "training_stats = []\n",
    "\n",
    "# to see the total time for all epoches\n",
    "total_t0 = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # to see results in proper way\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # to see the time for each epoch\n",
    "    t0 = time.time()\n",
    "\n",
    "    # cleaning loss for every epoch\n",
    "    total_train_loss = 0\n",
    "  \n",
    "    model.train() # not to perform training, it changes the mode\n",
    "\n",
    "    # each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # setting progress for each 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "             # printing progress of results\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # unpack the batch for each tensor with 'to' function \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # before backward pass, clearing previous gradients\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # forward pass which calculates the model on this training batch \n",
    "        # it returns loss on model outputs as logits\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # adding training loss for each steps \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # backward pass for gradient calculates \n",
    "        loss.backward()\n",
    "\n",
    "        # to prevent 'exploding gradient' problem, clip_grad_norm tool usage\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # AdamW optimizer for learning rate etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # new learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # mean of the all losses of training data\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # calculates time values \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    \n",
    "    # printing training results\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    # for printing in proper way \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval() # changing mode from train to evaluation\n",
    "\n",
    "    # for keeping values \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # loop for each batch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # unpack the batch for each tensor with 'to' function \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # no zero_grad bec. it is only needed for training part\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # forward pass and calculating logit predictions \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # keepong validation loss values\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # sent logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # for all batches keeping the accuracy\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # printing final accuracy value\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # mean loss for all batches\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # time measurement\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # keeping all results from this epoch\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "oAAQ3SY4zH-w",
    "outputId": "1d13f57a-437b-400c-c0cc-3a06025dac13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0:12:30</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0:12:28</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0:12:27</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0:12:26</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0:12:25</td>\n",
       "      <td>0:00:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.15         0.12           0.95       0:12:30         0:00:58\n",
       "2               0.10         0.15           0.95       0:12:28         0:00:58\n",
       "3               0.06         0.18           0.95       0:12:27         0:00:58\n",
       "4               0.03         0.25           0.95       0:12:26         0:00:58\n",
       "5               0.02         0.28           0.95       0:12:25         0:00:58"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 2)\n",
    "\n",
    "# results dataframe\n",
    "df_results = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# to see each epoch as index\n",
    "df_results = df_results.set_index('epoch')\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "rrDX2NIszSpL",
    "outputId": "cf1cc787-48e3-4efe-9d82-d4aba9a80194"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# plotting the learning curve\n",
    "plt.plot(df_results['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_results['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss for Each Epoch\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss Values\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weMCdBHitrOm"
   },
   "source": [
    "If I only look at accuracy values, I can believe every epoch is same, so looking to loss values is more accurate than accuracy. While the training loss is decreasing in each epoch, validation loss is increasing. It means that I trained my model too much long that over-fitting occured. I take results of this pre-trained model as 0.12 validation loss and 0.95 validation accuracy which is the results of first epoch. \n",
    "\n",
    "### Future Improvements for This Notebook\n",
    "\n",
    "- Setting epoch as 1, and use less batches than 64\n",
    "- Changing learning rate and epsilon value for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7_Pre-trained_BERT_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1221df09c2cd4c969db6546b76504886": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ed090fbe52c47f2b3038d48b05f9d0f",
       "IPY_MODEL_4388cda8b677438f8e89e1c9c4a386b7"
      ],
      "layout": "IPY_MODEL_ce69c8b1cbbf4ced800784099cd54c3e"
     }
    },
    "178db16194c04743a79c3c4144b658e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b4bbefd5982432fbf1cbe5b62ef6046": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "210c1900677048aca380294fc94182f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c86995273de742f29382438b10f1868b",
      "placeholder": "​",
      "style": "IPY_MODEL_660cfc5586714404b7c0dfe74f05b5e9",
      "value": " 440M/440M [00:08&lt;00:00, 51.9MB/s]"
     }
    },
    "218e8495c5374aeb944dcc4be1676b08": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "231c99254ea64e2cbf7a301b42f48da8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a65cbda0a4264b9bb05fbfe68dbc1b50",
       "IPY_MODEL_210c1900677048aca380294fc94182f7"
      ],
      "layout": "IPY_MODEL_218e8495c5374aeb944dcc4be1676b08"
     }
    },
    "2d1b451e4d4d46d78a463e18b9c87ae3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4388cda8b677438f8e89e1c9c4a386b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d1b451e4d4d46d78a463e18b9c87ae3",
      "placeholder": "​",
      "style": "IPY_MODEL_178db16194c04743a79c3c4144b658e5",
      "value": " 232k/232k [00:00&lt;00:00, 627kB/s]"
     }
    },
    "46a011e796e241c6807b48cdfed0ccd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4ae10cc6fe9a46a88c52ad56a93f80be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "660cfc5586714404b7c0dfe74f05b5e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a309c31776c490cba4e1a6b50a00310": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d21cffd4fe2f4b23b33660171258f1b3",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d41bf1f7e9c84326a417a039ba18b051",
      "value": 433
     }
    },
    "6ed090fbe52c47f2b3038d48b05f9d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b4bbefd5982432fbf1cbe5b62ef6046",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed9f67484fe747b0bab16c18b34e69b9",
      "value": 231508
     }
    },
    "71e8d4bceb874a6393ded5c0dd1bc623": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a65cbda0a4264b9bb05fbfe68dbc1b50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee5121d2e2aa44fd83be75842ce1d288",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46a011e796e241c6807b48cdfed0ccd1",
      "value": 440473133
     }
    },
    "a87b027c20344c588c9f24797b6c5196": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abbc04b4375e44a29d4e4b72e0a17ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a309c31776c490cba4e1a6b50a00310",
       "IPY_MODEL_ef904c4376514ea19c43d9c5bf00fc48"
      ],
      "layout": "IPY_MODEL_4ae10cc6fe9a46a88c52ad56a93f80be"
     }
    },
    "c86995273de742f29382438b10f1868b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce69c8b1cbbf4ced800784099cd54c3e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d21cffd4fe2f4b23b33660171258f1b3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d41bf1f7e9c84326a417a039ba18b051": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ed9f67484fe747b0bab16c18b34e69b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ee5121d2e2aa44fd83be75842ce1d288": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef904c4376514ea19c43d9c5bf00fc48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71e8d4bceb874a6393ded5c0dd1bc623",
      "placeholder": "​",
      "style": "IPY_MODEL_a87b027c20344c588c9f24797b6c5196",
      "value": " 433/433 [00:00&lt;00:00, 3.62kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
